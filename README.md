<h1 align="left">Rayane Tfeili</h1>
<p align="left"><b>Data & AI engineer</b> 路 <b>ML learner</b> 路 <b>Optimization  Transformers</b> 路 <b>Quant-curious</b></p>

<p align="left">
I work at the intersection of <b>modern deep learning</b> and <b>mathematical optimization</b>, with a growing focus on
<b>Transformers/LLMs</b> and <b>quantitative finance</b>. I enjoy building systems that are both <i>theoretically grounded</i>
and <i>practically deployable</i>.
</p>

---

##  Master thesis
<b>Convex Optimization Meets Attention: Mathematical Insights into Transformers and LLM</b>

My master thesis explores the mathematical foundations of modern deep learning with an emphasis on practical transformer architectures.
This work includes an implementation-from-scratch of a <b>convexified Transformer</b> model to study the effect of reformulated attention
mechanisms on model performance. The goal is to bridge theoretical optimization principles with practical deep learning architectures used
in modern AI systems.

### Thesis focus
- Attention mechanisms viewed through an <b>optimization lens</b> (reformulations, relaxations, constraints)
- A clean <b>from-scratch implementation</b> of a convexified Transformer variant
- Empirical study via <b>ablation</b>, <b>stability</b>, and <b>performance</b> metrics

---

##  What Im working on now
-  Convexification of attention / LLMs (thesis)
- З RAG project: 
-  Quant finance project: research/backtesting mindset + risk-aware metrics
-  Learning Rust 
---

##  Tools
- **Languages**: Python 路 R 路 SQL 路 Rust  
- **Libraries / Frameworks**: PyTorch 路 scikit-learn 路 PySpark 路 Dask  
- **DevOps / Containerization**: Docker

  ##  Interests
- **LLMs & Transformers**: attention mechanisms, scaling behavior, evaluation
- **Convex Optimization**
- **RAG system**
- **Quant finance**

---

##  Links
- LinkedIn: (https://www.linkedin.com/in/rayane-tfeili-539772246/?skipRedirect=true)
- Email: rayantfeili@gmail.com

---
