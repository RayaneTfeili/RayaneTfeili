<h1 align="left">Rayane Tfeili</h1>
<p align="left"><b>Data & AI engineer</b> Â· <b>ML learner</b> Â· <b>Optimization Ã— Transformers</b> Â· <b>Quant-curious</b></p>

<p align="left">
I work at the intersection of <b>modern deep learning</b> and <b>mathematical optimization</b>, with a growing focus on
<b>Transformers/LLMs</b> and <b>quantitative finance</b>. I enjoy building systems that are both <i>theoretically grounded</i>
and <i>practically deployable</i>.
</p>

---

## ğŸ“ Master thesis
<b>Convex Optimization Meets Attention: Mathematical Insights into Transformers and LLM</b>

My master thesis explores the mathematical foundations of modern deep learning with an emphasis on practical transformer architectures.
This work includes an implementation-from-scratch of a <b>convexified Transformer</b> model to study the effect of reformulated attention
mechanisms on model performance. The goal is to bridge theoretical optimization principles with practical deep learning architectures used
in modern AI systems.

### Thesis focus
- Attention mechanisms viewed through an <b>optimization lens</b> (reformulations, relaxations, constraints)
- A clean <b>from-scratch implementation</b> of a convexified Transformer variant
- Empirical study via <b>ablation</b>, <b>stability</b>, and <b>performance</b> metrics

---

## ğŸ”­ What Iâ€™m working on now
- ğŸ§  Convexification of attention / LLMs (thesis)
- ğŸ§© RAG project: 
- ğŸ’¹ Quant finance project: research/backtesting mindset + risk-aware metrics
- ğŸ¦€ Learning Rust 
---

## ğŸ› ï¸ Tools
- **Languages**: Python Â· R Â· SQL Â· Rust  
- **Libraries / Frameworks**: PyTorch Â· scikit-learn Â· PySpark Â· Dask  
- **DevOps / Containerization**: Docker

  ## ğŸ§  Interests
- **LLMs & Transformers**: attention mechanisms, scaling behavior, evaluation
- **Convex Optimization**
- **RAG**
- **Quant**

---

## ğŸ¤ Links
- GitHub: <b>@RayaneTfeili</b>
- LinkedIn: (https://www.linkedin.com/in/rayane-tfeili-539772246/?skipRedirect=true)
- Email: rayantfeili@gmail.com

---
